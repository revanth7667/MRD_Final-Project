---
title: "Los Angeles Crime Analysis"
author: "Group 13: Meixiang(md480), Revanth(rg361), Suim(sp699), Titus(tra29)"
format: pdf
editor: visual
echo: FALSE
output: FALSE
geometry: margin = 1.0cm
---

```{r}
#Load all the required packages and libraries
library(AER)
library(car)
library(caret)
library(caTools)
#library(corrplot)
#library(cvms)
library(dplyr)
library(geosphere)
#library(ggmap)
library(ggplot2)
#library(ggpubr)
library(gridExtra)
library(gtsummary)
#library(knitr)
#library(leaflet)
library(lubridate)
#library(maps)
library(Metrics)
#library(openintro)
library(pROC)
#library(purrr)
#library(stargazer)
#library(stringr)
library(tidyverse)

# Resolve package conflicts
#library(conflicted)
#conflict_prefer("filter", "dplyr")
#conflict_prefer("lag", "dplyr")
```

```{r}
#Load the data
df_precinct <- read.csv("../data/Precinct_Location.csv")
df <- read.csv('/Users/revanth/Documents/MIDS/Semester 1/MRD/Group Project/Datasets/crime.csv') #change this path later if required
```

```{r}
str(df)
```

```{r}
#Date Related Cleanings
#Clean the Data Columns and create difference
df$DATE.OCC <- substr(df$DATE.OCC,1,10) 
df$DATE.OCC <- as.Date(df$DATE.OCC, "%m/%d/%Y")

df$Date.Rptd <- substr(df$Date.Rptd,1,10) 
df$Date.Rptd <- as.Date(df$Date.Rptd, "%m/%d/%Y")

df$rpt_diff <- df$Date.Rptd- df$DATE.OCC
```

```{r}
#Dropping data after Sept-2023 as this may give outlier as seen during EDA, adjust to Oct-01 if required
df <- df[df$DATE.OCC < as.Date("09/01/2023", "%m/%d/%Y"), ]
```

```{r}
#Calculate Year, Month, Day and Weekday
df$dt_year = format(df$DATE.OCC,"%Y")
df$dt_month = as.factor(format(df$DATE.OCC,"%m"))
df$dt_day = format(df$DATE.OCC,"%d")
df$dt_weekday <- as.factor(wday(df$DATE.OCC, label=TRUE))
```

```{r}
#Calculate Distance to Precinct in Miles
df <- df %>% left_join( df_precinct[,c("precinct_code","precinct_lat","precinct_long")], 
        by=c('AREA'='precinct_code'))

df$dist_to_precinct <- distHaversine(df[,c("LON","LAT")],df[,c("precinct_long","precinct_lat")])*0.00062137
```

```{r}
#create text column for crime_type
df$crime_type <- as.factor(ifelse(df$Part.1.2==1,"serious","non-serious"))
```

```{r}
#clean time 
df$time_hr <- as.integer(substr(str_pad(as.character(df$TIME.OCC),4,pad=0),1,2))
```

```{r}
#create binary column if weapon is involved
df$weapons_binary <- as.factor(ifelse(is.na(df$Weapon.Used.Cd) | (df$Weapon.Used.Cd==""),0,1))
```

```{r}
#Performing the following Cleanings as per Suim's Logistic Regression requirements

#Cleaning Descent
df$Vict.Descent[df$Vict.Descent == ""] <- "X"
df$Vict.Descent[df$Vict.Descent == "-"] <- "X"
df$Vict.Descent <- as.factor(df$Vict.Descent)

#Cleaning Gender 
df$Vict.Sex[df$Vict.Sex == ""] <- "X"
df$Vict.Sex[df$Vict.Sex == "-"] <- "X"
df$Vict.Sex <- as.factor(df$Vict.Sex)
```

# 1. Abstract

This analysis delves into the intricate dynamics of crime in Los Angeles, employing an extensive dataset from the Los Angeles Police Department (LAPD), covering incidents from 2020 to present day. Utilizing Logistic and Poisson regression models, our research aims to unearth the pivotal factors influencing the seriousness of crime commited and predict the number crime occurrences in a given area and time respectively. Key findings from the first research objective indicate a relationship between factors like victim demorgraphic and time of day on the seriousness of crime committed. Findings from the second research objective indicate significant temporal and spatial variations in crime rates, offering valuable insights for law enforcement and public safety strategies. This study not only enhances the understanding of crime patterns but also aids in resource allocation and preventive measures.

# 2. Introduction

Crime, a complex and multifaceted social issue, demands meticulous investigation to inform and enhance public safety measures. This report capitalizes on a detailed dataset from the Los Angeles Police Department(LAPD), encompassing a vast array of reported crimes recorded from 2020 to the present day and updated by the LAPD on a `weekly` basis. The dataset offers a rich tapestry of information, offering a comprehensive view of the crime landscape in Los Angeles.

As of **13-Oct-2023**, the Dataset has **815,882** observations of **28** variables and the information provided can broadly be classified into the following categories:

1.  `Location`: Latitude, Longitude, Area, Street, District
2.  `Victim Demographic`: Age, Gender, Ethnicity
3.  `Crime Description`: Type of Crime, Investigation Outcomes, Weapon Usage
4.  `Date and Time`: Date Reported, Date Occurred, Time Occurred
5.  `Identifier/Classifier`: Crime Record Identifier, Mocodes

Central to our analysis are two pivotal research questions: First, we explore the determinants impacting the severity of crimes, a query crucial for preventive strategies and public awareness. Second, we aim to predict crime frequencies in specific areas and timeframes, a task vital for strategic law enforcement deployment and community safety initiatives. By employing logistic regression, we seek to understand the factors that significantly influence the seriousness of crimes. Concurrently, through Poisson regression modeling, we aim to forecast crime occurrences, accounting for a variety of predictors such as time, date, and geographic factors.Our investigation is not merely an academic exercise but a crucial endeavor to discern patterns and predictors within the urban landscape of Los Angeles. The insights gleaned from this analysis we're hoping can be instrumental in helping law enforcement agencies, policymakers, and the community at large, enabling a data-driven approach to crime prevention and optimized policing efforts. Furthermore, the study contributes to a broader understanding of criminal behavior, aiding in the development of effective community outreach programs and mitigating risk factors associated with crime.

# 3. Methodology

Since the Dataset is dynamic in nature and updated on a weekly basis, a fixed snapshot of the data as of **13-Oct-2023** was used throughout the research process to prevent any errors or fluctuations of the outcomes and results.

## Data Cleaning & EDA

The following cleaning and processing steps were performed on the data before it was used for the analysis and modelling:

1.  Distance to precinct: using the Latitude (LAT) and Longitude (LON) columns from the dataset and publicly available co-ordinates of the precincts in Los Angeles we calculated the distance between the spot of crime occurrence and the precinct it was reported in, to understand if there was a relationship between the distance and the seriousness of crime.
2.  Data Type Conversion: Columns were converted to their relevant Datatypes e.g: Conversion of Date related columns to Datetime, Victim Sex, Gender as Factors etc.
3.  Bucketing: Columns like Time of crime occurrence were bucketed in order to decrease the number of unique levels and avoid over-fitting
4.  Imputation: Empty values (systemic) in certain columns of interest like Victim Gender and Ethnicity were filled or combined with place holder values (e.g. "X") so that they can be used for analysis and modelling
5.  Time Restriction: Since the data is dynamic in nature and the most recent crimes may not be reported yet, the time-frame for analysis was restricted and we considered only data up to **31-Aug-2023** so that there is no skewing of the results
6.  Helper Columns: New columns such as number of days between crime occurrence and reporting, Month, Weekday etc. were calculated from the columns present in the dataset.

Post the restrcition of the Time-frame for analysis, The number of observations reduced to **794,388**, a reduction of **21,494** (\~2.5%) observations.

Exploratory Data Analysis (EDA) was performed to understand the trends and relationship between the crime occurrences and the following factors:

1.  Temporal: Time of occurrence, weekday, month
2.  Precinct: Number of crimes reported, distance to precinct
3.  Victim Demographic: Age, Ethnicity, Gender
4.  General: proportion of crime types, daily rate

## Research Question 1

Question: **"What are the strongest indicating factors that influence the seriousness of crime committed"**

The **binary** outcome variable "crime type" was created by converting the "Part 1 2" column from the original dataset which indicates the seriousness of crime committed, the following mapping was performed 1-"serious", 2-"non-serious" and the result was saved as as a factor.

**Logistic Regression** was chosen for this analysis since it is well-suited for binary outcomes such as 'serious' and 'non-serious'. Logistic regression is also particularly effective for inference since the output is easily interpretable, allowing us to identify factors that influence the seriousness of crimes committed.

Prior to model construction, it was essential to eliminate rows with blank values or "-" for two specific variables: victim sex and victim descent. These missing values were impractical to impute and were replaced with 'unknown' during model configuration, due to the dataset's large size. Subsequently, we removed all 'unknown' values from each variable.

Based on a priori selection, we included factors like time of occurrence, distance to the precinct, demographic information (gender, age, descent), and weapon usage, hypothesizing that these significantly impact crime seriousness. Regarding collinearity, the Variance Inflation Factor (VIF) values for each variable were around 1, indicating stability in the model.

## Research Question 2

Question: **"What is the predicted number of crimes for a given area and time period?"**

Each row of our dataset is an individual crime incident, to get the count of crimes which is required for answering this research question, we aggregated the data at a Location(Area)-Month-Weekday level and got the count of crimes at this level.

**Poisson regression** was chosen for this analysis as it is as optimal approach for modeling count data and the output can be interpreted easily. we aim to predict the frequency of crimes in Los Angeles with precision and statistical accuracy, leveraging the model's suitability for such discrete outcome variables. This model allows us to integrate time, date, and location variables, capturing the essence of crime occurrences over various periods and areas. By employing a Poisson regression model, we aim to dissect the frequency of crimes within the multifaceted urban setup of Los Angeles, identifying temporal and spatial hotspots of criminal activity.

Given the prediction nature, we've narrowed down our model to the following variables, and undergone **stepwise forward** selection and **cross-validation** to refine and optimize the predictive performance.

List of Predictor Variables

-   Independent Variables

    \- Date: Month and Weekday

    \- Geographic Factor: Area

-   Outcome Variable

    \- Count of Crimes (Generated by grouping the data set by the selected predictor variables)

# 4. Results

## 4.1 Exploratory Data Analysis

The following observations were made during the EDA process.

About **60%** of the crimes committed are serious crimes, and on and average **593** crimes occur in a day with with a few days having very high number of crimes.

```{r}
plot_type <-ggplot(df, aes(x = crime_type, fill = crime_type)) +
  geom_bar() +
  labs(title = "Distribution of Crime Types", x = "Crime Types", y = "Count") +
  geom_text(aes(label = after_stat(count)), stat = "count", vjust = 1.5) +   # Adjusted the vertical position of the text
  #scale_fill_manual(values = c("non-serious" = "#88DD88", "serious" = "#FF6666")) +  # Dimmed shades of green and red
  theme(legend.position = "none",      # Remove the legend
        plot.title = element_text(hjust = 0.5))  # Center the title
plot_type
```

```{r}
daily_crime_count <- df %>%
  group_by(DATE.OCC) %>%
  summarize(Count = n()) %>%
  arrange(DATE.OCC)

# Calculate the average value
avg_value <- mean(daily_crime_count$Count)

plot_day_avg <- ggplot(data = daily_crime_count, aes(x = DATE.OCC, y = Count)) +
  geom_line(color = "#4A90E2") +  
  labs(title = "Daily Crime Counts", x = "Date The Crime Occurred", y = "Count") +
  geom_hline(yintercept = avg_value, linetype = "dashed", color = "red") +
  geom_text(aes(x = max(DATE.OCC), y = avg_value + max(Count) * 0.02, 
                label = paste("Average:", round(avg_value, 0))), 
            color = "red", hjust = 0.78) +
  theme(legend.position = "none",      
        plot.title = element_text(hjust = 0.5),   
        axis.title.x = element_text(size=12), 
        axis.title.y = element_text(size=12))
```

```{r, output = TRUE, fig.align="center", fig.height=3, fig.width=7}
grid.arrange(plot_type,plot_day_avg)
```

There is a minor variation in the proportion and count of crimes when compared on a monthly and weekday level. The variation is more prominent at the 'Time of Day' level.

```{r}
#Time, Day and Month
q1 <- ggplot(df, aes(x=time_hr, fill=crime_type))+
  geom_histogram(binwidth = 1, position="dodge", alpha=0.7)+
  labs(title = NULL, x = "Time of Day (24-Hour)", y = "Count") +
  scale_fill_discrete(name="Crime Type")  # Set legend title

q2 <- ggplot(df, aes(x=dt_weekday, fill=crime_type))+
  geom_bar(position="dodge")+
  labs(title = NULL, x = "Weekday", y = "Count") +
  theme(legend.position="none")

q3 <- filter(df, dt_year != 2023) %>%
  ggplot(aes(x=dt_month, fill=crime_type))+
  geom_bar(position="dodge")+
  labs(title = NULL, x = "Month", y = 'Count')+
  theme(legend.position="none")
```

```{r, output = TRUE, fig.align="center", fig.height=3, fig.width=7}
# Adjust the plot margins
margin_adjustment <- theme(plot.margin = margin(5, 5, 5, 5, "pt"))
q1 <- q1 + margin_adjustment
q2 <- q2 + margin_adjustment
q3 <- q3 + margin_adjustment

# Define a layout matrix
# The numbers in the matrix correspond to the plots:
# 1 = q3, 2 = q2, 3 = q1
# The layout matrix arranges q3 and q2 side by side in the first row,
# and q1 (stretched) in the second row.
layout_matrix <- rbind(c(1, 2),
                      c(3, 3)) # Stretching q1 across the width of the grid

# Combine the plots using the custom layout
grid.arrange(q3, q2, q1, layout_matrix = layout_matrix, 
             top="Crime Distribution by Month, Weekday, and Time of Day")
```

There is a minor variation in the proportion and count of crimes at different precincts.

```{r, output = TRUE, fig.align="center", fig.height=3, fig.width=7}
# First plot
p1 <- ggplot(df, aes(x = AREA.NAME, fill = crime_type)) +
  geom_bar(position = "dodge", alpha = 0.7) +
  labs(x = "Precinct", y = "Count", fill = "Crime Type") +  # Removed title to avoid duplication
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))  # Rotate x-axis labels

# Second plot
p2 <- df %>% filter(dist_to_precinct < 10) %>%
  ggplot(aes(x = dist_to_precinct, fill = crime_type)) +
  geom_histogram(binwidth = 1, position = "dodge", alpha = 0.7) +
  labs(x = "Distance (in miles)", y = "Count", fill = "Crime Type")  # Removed title to avoid duplication

# Combine the two plots with a single title
grid.arrange(p1, p2, ncol = 1, top = "Crime Count and Distance to Precinct")
```

The victim demographic data - 'Age', 'Sex' and 'Descent' have a large number of missing values(systematic). There is a clear observable variation of the number and proportion of the type of crimes with respect to these variables. (Refer to Appendix for Meaning of Letter Codes)

```{r, output = TRUE, fig.align="center", fig.height=3, fig.width=7}
# Assign plots to variables
p1 <- ggplot(df, aes(x = Vict.Age, fill = as.factor(crime_type))) +
  geom_histogram(binwidth = 10, position = "dodge", color = "black") +
  labs(title = NULL, 
       x = "Age", 
       y = "Count", 
       fill = "Crime Type") +
  theme(legend.position="none")

p2 <- ggplot(df, aes(x = Vict.Sex, fill = as.factor(crime_type))) +
  geom_bar(position = "dodge", color = "black") +
  labs(title = NULL, 
       x = "Sex", 
       y = "Count") +  # Removed the fill legend title
  theme_minimal() +
  theme(legend.position="none")  # Remove the legend

p3 <- ggplot(df, aes(x = Vict.Descent, fill = as.factor(crime_type))) +
  geom_bar(position = "dodge", color = "black") +
  labs(title = NULL, 
       x = "Descent", 
       y = "Count", 
       fill = "Crime Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

# Adjust the plot margins
margin_adjustment <- theme(plot.margin = margin(5, 5, 5, 5, "pt"))
p1 <- p1 + margin_adjustment
p2 <- p2 + margin_adjustment
p3 <- p3 + margin_adjustment

# Define a layout matrix similar to the previous one, but for the new plots
# 1 = p1, 2 = p2, 3 = p3
# The layout matrix arranges p1 and p2 side by side in the first row,
# and p3 (stretched) in the second row.
layout_matrix_new <- rbind(c(1, 2),
                           c(3, 3)) # Stretching p3 across the width of the grid

# Use grid.arrange to display the plots with the custom layout
grid.arrange(p1, p2, p3, layout_matrix = layout_matrix_new, 
             top="Distribution by Victim Age, Sex, and Descent")
```

```{r}
str(df)
```

```{r}
#Creating a Clean Df only the columsn which will be used ahead

#list of columns to keep
column_keep <- c("crime_type","time_hr", "Vict.Age", "Vict.Sex", "Vict.Descent", "dist_to_precinct", "weapons_binary", "AREA.NAME", "dt_weekday", "dt_month")

#create new df with these columns
df2 <- df[,names(df) %in% column_keep]

str(df2)
```

## 4.2 : Logistic Regression

The model was refined using forward stepwise selection and cross-validation. The goodness of fit was indicated by a null deviance of 829808, leading to a significant model improvement with an AIC of 829856. However, the McFadden's pseudo R^2^ value of 0.0367, though low, suggests limited explanatory power.

The model's accuracy was approximately 60.07%, not sufficiently high to be definitive. A Kappa value of 0.1951, while slightly better than chance, also indicates modest performance. Sensitivity and specificity are crucial in evaluating the model. Sensitivity (true positive rate) effectively identifies actual serious cases, while specificity (true negative rate) accurately identifies non-serious cases. These metrics collectively offer a comprehensive view of the model's ability to distinguish between serious and non-serious cases.

The model's insights include:

-   Time hours occurred: Night or early morning crimes were more likely to be serious.

-   Distance to precinct: Closer proximity to certain precincts correlated with serious crimes.

-   Demographic Information: Men and younger individuals were more often victims of serious crimes, with certain racial groups being more susceptible. Among the various descents, individuals of Cambodian, Filipino, Japanese, Korean, Vietnamese, and Asian Indian descent were more likely to be victims of serious crimes compared to other racial groups.

-   Weapon Used: The presence of a weapon did not significantly increase the likelihood of a crime being serious. Instead, crimes committed without a weapon had a higher probability of occurring.

```{r}
#creating dataframe for logistic regression
df_logi <- df2[,!names(df2) %in% c("AREA.NAME", "dt_weekday", "dt_month")]
str(df_logi)
```

```{r}
model_logi <- glm(crime_type ~ ., data=df_logi, family="binomial")
```

```{r}
summary(model_logi)
```

```{r}
exp(coef(model_logi))
```

```{r}
model_logi_1 <- glm(crime_type ~ time_hr + Vict.Age + Vict.Sex + dist_to_precinct + weapons_binary, family = "binomial", data = df_logi)
```

```{r}
model_logi_2 <- glm(crime_type ~ time_hr + Vict.Age + dist_to_precinct + weapons_binary, family = "binomial", data = df_logi)
```

```{r}
summary(model_logi_1)
```

```{r}
summary(model_logi_2)
```

```{r}
model_logi_interaction <- glm(crime_type ~ time_hr + Vict.Age + Vict.Sex + Vict.Descent + Vict.Sex * Vict.Descent + dist_to_precinct + weapons_binary, family = "binomial", data = df_logi)
```

```{r}
summary(model_logi_interaction)
```

```{r}
deviance_value <- deviance(model_logi)
deviance_value_1 <- deviance(model_logi_1)
deviance_value_2 <- deviance(model_logi_2)
deviance_value
deviance_value_1
deviance_value_2
```

```{r}
aic_value_base <- AIC(model_logi)
aic_value_alt_1 <- AIC(model_logi_1)
aic_value_alt_2 <- AIC(model_logi_2)
aic_value_base
aic_value_alt_1
aic_value_alt_2
```

```{r}
null_model <- glm(crime_type ~ 1, data = df_logi, family = "binomial")
null_deviance <- deviance(null_model)
model_deviance <- deviance(model_logi)
mcfaddens_pseudo_r2 <- 1 - (model_deviance / null_deviance)
mcfaddens_pseudo_r2
```

```{r}
roc_plot = roc(factor(df_logi$crime_type), fitted(model_logi))

# basic ROC plot
plot(roc_plot, main = "ROC Curve")

# calculate threshold
coords <- coords(roc_plot, "best")

# add  threshold and AOC value
text(x = coords$specificity, y = coords$sensitivity, labels = paste("Threshold:", coords$threshold), pos = 4, cex = 0.8)
text(x = coords$specificity, y = coords$sensitivity, labels = paste("AUC:", round(auc(roc_plot), 2)), pos = 2, cex = 0.8)
```

```{r}
predicted_probabilities <- fitted(model_logi)
predicted_classes <- ifelse(predicted_probabilities > 0.616, "1", "0")
```

```{r}
crime_type_numeric <- as.numeric(factor(df_logi$crime_type, levels = c("non-serious", "serious"))) - 1
```

```{r}
crime_type_factor <- factor(crime_type_numeric, levels = c("0", "1"))
predicted_factor <- factor(predicted_classes, levels = c("0", "1"))
```

```{r}
conf_matrix <- confusionMatrix(predicted_factor, crime_type_factor, positive = "1")
conf_matrix
```

```{r}
#california_map <- map_data("county", "california")
```

```{r}
vif(model_logi)
```

```{r}
roc_plot = roc(df_logi$crime_type, fitted(model_logi))

# basic ROC plot
plot(roc_plot, main = "ROC Curve")

# calculate threshold
coords <- coords(roc_plot, "best")

# add  threshold and AOC value
text(x = coords$specificity, y = coords$sensitivity, labels = paste("Threshold:", coords$threshold), pos = 4, cex = 0.8)
text(x = coords$specificity, y = coords$sensitivity, labels = paste("AUC:", round(auc(roc_plot), 2)), pos = 2, cex = 0.8)
```

```{r}
df_logi$predprobs <- predict(model_logi, type = "response")
plot(df_logi$predprobs, xlab = "Observations", ylab = "Predicted Probability", main = "Predicted Probabilities")
```

## Model 2 : Poisson

With forward step wise selection and cross validation methods to improve model, the deviance measures goodness of fit, with a null deviance of 35064 and a residual deviance of 5204, indicating a substantial improvement in model fit, resulting in an AIC value of 15356 with 4 Fisher Scoring iterations.

Our preliminary evaluation of the model's predictive accuracy, gauged through the RMSE, yielded a value of 42.59 to estimate the average difference between the predicted and observed crime counts. Additionally, the R-squared value of 0.82 reflected a strong linear relationship between the observed and predicted crime counts, explaining a substantial proportion of the variance in the crime data.

Preliminary examination of the model outputs suggests distinct spatial and temporal crime patterns.

-   **Day of the Week Matters:**Different weekdays impact crime differently. For example, crimes are more likely on weekends compared to others weekdays, and these differences are confirmed by the p-values associated with these coefficient.

-   **Monthly Trends in Crime:**Crime rates vary by month. Some months, like February and September, see fewer crimes, while others, like July and August, experience more. These patterns are statistically significant.

-   **Geographical Differences:**Crime rates differ by police area. Some areas have higher crime rates (e.g., "Southwest"), while others have lower rates (e.g., "Hollywood"). These variations are statistically significant, providing insights into local crime trends.

However, with a dispersion value of 4.07, it indicates potential problems with the model assumptions, likely due to factors such as Population Heterogeneity and Model Misspecification. Zero-Inflation and Correlation Among Observations have been ruled out. To address these issues, consider switching to a negative binomial model for better handling over-dispersion and troubleshoot Population Heterogeneity and Model Misspecification to improve the model.

Plot the predictions alongside observed counts, depicts the pattern of the number of crime varies from month to month weekday and weekends.

```{r}
df_pois <- df2[,names(df2) %in% c("AREA.NAME", "dt_weekday","dt_month") ]
str(df_pois)
```

```{r}
df_daily <- df_pois %>%
  group_by(dt_weekday,dt_month,AREA.NAME)%>%
  summarise(count = n())
```

We can skip the below part since we have a similar plot in the EDA section, moght help save space

```{r}
#| fig-width: 5.5
#| fig-height: 2
library(gridExtra)
library(ggplot2)
library(dplyr)

# Create the histogram plot
p1 <- ggplot(df_daily, aes(x = count, y = ..density..)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Crime Counts (Proportions)",
       x = "Crime Count",
       y = "Proportion") +
  theme_minimal()

# Aggregate counts by weekday across all areas and months
weekday_counts <- df_daily %>%
  group_by(dt_weekday) %>%
  summarise(total_count = sum(count))

# Create the bar chart for weekday counts
p2 <- ggplot(weekday_counts, aes(x = dt_weekday, y = total_count, fill = dt_weekday)) +
  geom_bar(stat = "identity") +
  labs(title = "Crime Counts Weekday",
       x = "Day of the Week",
       y = "Total Count Crimes") +
  theme_minimal() +
  theme(legend.position = "none")

# Adjust plot margins
margin_adjustment <- theme(plot.margin = margin(5, 5, 5, 5, "pt"))
p1 <- p1 + margin_adjustment
p2 <- p2 + margin_adjustment

# Define a layout matrix to arrange the plots
# Here we arrange p1 on the left and p2 on the right in a single row
layout_matrix <- matrix(c(1, 2), nrow = 1)

# Use grid.arrange to display the plots side by side
grid.arrange(p1, p2, layout_matrix = layout_matrix, 
             top="Crime Count Distributions")
```

```{r}
#cross validation setup
set.seed(123)

split<-sample.split(df_daily$count,SplitRatio = 0.7)
train<-subset(df_daily,split==TRUE)
test<-subset(df_daily,split ==FALSE)
```

```{r}
#fitting the model 
model_pois <- glm(count ~ . , data = train, family = poisson)
model_forward <- step(model_pois, direction ="both")
summary(model_forward)
```

```{r}
#Testing the model, evaluation 
dispersiontest(model_forward)
```

```{r}
# Predicting the outputs
predictions <- predict(model_forward, newdata = test, type='response')

# Add predictions to the test dataset
test$predict <- predictions

# Display the first few rows of the test dataset with predictions
head(test)
```

```{r}
# Summary of the model with exponentiated coefficients, confidence intervals, and p-values
exp_coef <- exp(coef(model_forward))
conf_int <- exp(confint(model_forward))
p_values <- summary(model_forward)$coefficients[, 4]

# Combine the exponentiated coefficients with their corresponding confidence intervals and p-values
results_summary <- cbind(exp_coef, conf_int, p_values)
```

```{r}
rmse(test$count, test$predict)
```

```{r}
#To calcualte the R2
rsq <- function (x, y) cor(x, y) ^ 2
rs <- rsq(test$count, test$predict)
rs
```

```{r}
summary(test$count)
```

```{r}
summary(test$predict)
```

```{r, output = TRUE, fig.align="center", fig.height=3, fig.width=7}
ggplot(test, aes(x = dt_month , y = predict, colour = dt_weekday)) +
geom_point(aes(y = count), alpha = 0.5,
position = position_jitter(h = 0.2)) +
geom_line() + labs(x = "", y = "Number of crimes",
colour = "Weekday") + ggtitle("Predicted Number of crime by month")
```
